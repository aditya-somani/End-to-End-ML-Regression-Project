{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41e68a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f64ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load datasets (train + eval)\n",
    "\n",
    "train_df = pd.read_csv(r'C:\\Users\\H.P\\Desktop\\Housing Regression MLE\\data\\processed\\feature_engineered_train.csv')\n",
    "eval_df = pd.read_csv(r'C:\\Users\\H.P\\Desktop\\Housing Regression MLE\\data\\processed\\feature_engineered_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21be13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 3. Drop high VIF features (both train + eval)\\nhigh_vif_features = [\\n    \"median_sale_price\" #highest correlation to \\'price\\' => data leakage\\n]\\ntrain_df.drop(columns=high_vif_features, inplace=True)\\neval_df.drop(columns=high_vif_features, inplace=True)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 3. Drop high VIF features (both train + eval)\n",
    "high_vif_features = [\n",
    "    \"median_sale_price\" #highest correlation to 'price' => data leakage\n",
    "]\n",
    "train_df.drop(columns=high_vif_features, inplace=True)\n",
    "eval_df.drop(columns=high_vif_features, inplace=True)\n",
    "'''\n",
    "\n",
    "# So I am skipping this step because I know my I won't be getting best result by these models. And my\n",
    "# rest complex models are naturally immune to multicolinearity issues. Otherwise I would have dropped\n",
    "# high VIF features here in both train and eval datasets. In case I considered this a serious candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f7f05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define target & features\n",
    "\n",
    "target = \"price\"\n",
    "X_train = train_df.drop(columns=[target])\n",
    "y_train = train_df[target]\n",
    "\n",
    "X_eval = eval_df.drop(columns=[target])\n",
    "y_eval = eval_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7349c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Standardization (fit on train, transform eval)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_eval_scaled = scaler.transform(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00dba5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      " MAE: 57163.4322166157\n",
      " RMSE: 122396.97072119515\n",
      " R²: 0.8842285774323847\n"
     ]
    }
   ],
   "source": [
    "# 6. Train & Evaluate Models\n",
    "\n",
    "# --- Linear Regression ---\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_eval_scaled)\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(\" MAE:\", mean_absolute_error(y_eval, y_pred_lr))\n",
    "print(\" RMSE:\", np.sqrt(mean_squared_error(y_eval, y_pred_lr)))\n",
    "print(\" R²:\", r2_score(y_eval, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3f4a90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Regression:\n",
      " MAE: 57163.535494486576\n",
      " RMSE: 122398.36753582611\n",
      " R²: 0.8842259350117647\n"
     ]
    }
   ],
   "source": [
    "# --- Ridge Regression ---\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge.predict(X_eval_scaled)\n",
    "\n",
    "print(\"\\nRidge Regression:\")\n",
    "print(\" MAE:\", mean_absolute_error(y_eval, y_pred_ridge))\n",
    "print(\" RMSE:\", np.sqrt(mean_squared_error(y_eval, y_pred_ridge)))\n",
    "print(\" R²:\", r2_score(y_eval, y_pred_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b91416c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso Regression:\n",
      " MAE: 57602.534010093914\n",
      " RMSE: 122701.30818468778\n",
      " R²: 0.8836521353087576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\H.P\\Desktop\\Housing Regression MLE\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.501e+15, tolerance: 5.268e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# --- Lasso Regression ---\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "y_pred_lasso = lasso.predict(X_eval_scaled)\n",
    "\n",
    "print(\"\\nLasso Regression:\")\n",
    "print(\" MAE:\", mean_absolute_error(y_eval, y_pred_lasso))\n",
    "print(\" RMSE:\", np.sqrt(mean_squared_error(y_eval, y_pred_lasso)))\n",
    "print(\" R²:\", r2_score(y_eval, y_pred_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f31b0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ElasticNet Regression:\n",
      " MAE: 57192.776489120704\n",
      " RMSE: 124255.01302067707\n",
      " R²: 0.880686971465787\n"
     ]
    }
   ],
   "source": [
    "# --- ElasticNet ---\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic.fit(X_train_scaled, y_train)\n",
    "y_pred_elastic = elastic.predict(X_eval_scaled)\n",
    "\n",
    "print(\"\\nElasticNet Regression:\")\n",
    "print(\" MAE:\", mean_absolute_error(y_eval, y_pred_elastic))\n",
    "print(\" RMSE:\", np.sqrt(mean_squared_error(y_eval, y_pred_elastic)))\n",
    "print(\" R²:\", r2_score(y_eval, y_pred_elastic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245f373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "housing-regression-mle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
